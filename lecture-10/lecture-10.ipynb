{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative solutions of equations\n",
    "\n",
    "Consider the following simple problem: assume we have a real valued function $f(x)$ and we would like to solve the equation \n",
    "\n",
    "$$ f(x) = c $$\n",
    "\n",
    "for some constant $c$.  Let us also assume that we made a guess $f(x_0) = c$.  Of course, unless we are extremely lucky, we are not going to hit the result. So, there will be an error:\n",
    "\n",
    "$$ f(x_0) = c + \\delta $$\n",
    "\n",
    "Now, using this error, let us improve our guess:\n",
    "\n",
    "$$ f(x_0) - \\delta = c $$\n",
    "\n",
    "But we want $\\delta$ to effect $x_0$.  Assuming we have a *local inverse* we get\n",
    "\n",
    "$$ f^{-1}(f(x_0) - \\delta) = f^{-1}(c) = a $$\n",
    "\n",
    "where $a$ is the solution we need to find.  Now, let us write the first order Taylor approximation for the left hand side:\n",
    "\n",
    "$$ x_0 - (f^{-1})'(x_0) \\cdot \\delta \\approx a $$\n",
    "\n",
    "and we know that $(f^{-1})'(x_0) = \\frac{1}{f'(x_0)}$\n",
    "\n",
    "So, our next best guess is going to be\n",
    "\n",
    "$$ x_1 = x_0 - \\frac{\\delta}{f'(x_0)} $$\n",
    "\n",
    "If we convert this formula into an iterative approximation, we get\n",
    "\n",
    "$$ x_{n+1} = x_n - \\frac{\\delta_n}{f'(x_n)} $$\n",
    "\n",
    "where $\\delta_n = f(x_n) - c$\n",
    "\n",
    "This algorithm is called [Newton-Raphson algorithm](https://en.wikipedia.org/wiki/Newton%27s_method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Solve(f, c, x0, eta=1e-2, n=10000):\n",
    "    for i in range(n):\n",
    "        delta = f(x0) - c\n",
    "        der = (f(x0+eta/2) - f(x0-eta/2))/eta\n",
    "        x1 = x0-delta/(der+eta*np.random.rand())\n",
    "        if(abs(x0-x1)<eta):\n",
    "            break\n",
    "        else: \n",
    "            x0 = x1\n",
    "    return([i,x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn(x):\n",
    "    y = x*x\n",
    "    return(1.0 + math.cos(y+0.2)+math.log(0.24+y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8, 1.7734318154171977, 1.24]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Solve(fn,1.24,1.0,1e-10)\n",
    "[x[0],x[1],fn(x[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple variable case (steepest descent)\n",
    "\n",
    "We can develop a similar algorithm for functions with several variables. That algorithm is called [steepest descent algorithm](https://ocw.mit.edu/courses/mathematics/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/lecture-notes/MIT18_409F09_scribe21.pdf):\n",
    "\n",
    "Given a function $F(x_1,\\ldots,x_n)$ the direction in which $F$ changes the most is the gradient of $F$ which is defined as\n",
    "\n",
    "$$ \\nabla \\cdot F = \\left(\\frac{\\partial F}{\\partial x_1},\\ldots,\\frac{\\partial F}{\\partial x_n}\\right) $$\n",
    "\n",
    "So, if we start with an initial guess $a^{(0)}$ for $F(a_1^{(0)},\\ldots,a_n^{(0)}) = c$, the update rule is going to be\n",
    "\n",
    "$$ a^{(m+1)} = a^{(m)} - \\eta \\left(\\nabla\\cdot F\\right)(a_1^{(m)},\\ldots,a_n^{(m)}) $$\n",
    "\n",
    "where $\\eta$ is called *the learning rate*.\n",
    "\n",
    "![](images/steepest_descent.png)\n",
    "\n",
    "(Image is taken from [\"Learning-Based Auditory Encoding for Robust Speech Recognition\" by Yu-Hsiang Bosco Chiu, Bhiksha Raj, and Richard M Stern](https://www.researchgate.net/figure/An-example-of-steepest-descent-optimization-steps_fig2_220655581)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(f,x,eta=1e-4):\n",
    "    def delta(i,j): \n",
    "        if(i==j): return(1) \n",
    "        else: return(0)\n",
    "    def der(i,eta=1e-4):\n",
    "        vec = np.array([delta(i,j) for j in range(len(x))])\n",
    "        x1 = x + vec*eta/2\n",
    "        x0 = x - vec*eta/2\n",
    "        return((f(x1) - f(x0) + eta*np.random.rand())/eta)\n",
    "    return(np.array([der(i,eta) for i in range(len(x))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSolve(f,c,x0,eta=1e-4,n=1000):\n",
    "    for i in range(n):\n",
    "        delta = f(x0) - c\n",
    "        x1 = x0 - delta*eta*grad(f,x0,eta)\n",
    "        err = np.linalg.norm(x1-x0)\n",
    "        if(err < eta):\n",
    "            break\n",
    "        else:\n",
    "            x0 = x1\n",
    "    return([i,x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, array([0.0005877 , 0.00043165])]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def g(x):\n",
    "    y = x[0]*x[0]+x[1]*x[1]\n",
    "    return(1.0+math.atan(y)+math.log(1.0+y))\n",
    "\n",
    "MSolve(g,3.0,[0.0,0.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The perceptron\n",
    "\n",
    "Consider the following problem now:\n",
    "\n",
    "Assume we have a collection of data points $(x^{(i)},y^{(i)})$ that satisfy a relationship of the form\n",
    "\n",
    "$$ y^{(i)} - f(\\alpha\\cdot x^{(i)} + \\beta) \\sim N(0,\\sigma) $$\n",
    "\n",
    "where $f\\colon\\mathbb{R}\\to\\mathbb{R}$ is a real valued function of a single variable, $\\alpha$ and $x^{(i)}$ are vectors in an inner product space and $\\beta$ is a scalar.  Our task is to find the best fitting pair $(\\alpha,\\beta)$ such that \n",
    "\n",
    "$$ \\sum_i (y^{(i)} - f(\\alpha\\cdot x^{(i)} + \\beta))^2 $$\n",
    "\n",
    "is minimized.\n",
    "\n",
    "This is a generalization of the [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) algorithm we covered in [Lecture 7](../lecture-7/lecture-7.ipynb).  In the logistic regression case $f(x) = \\frac{1}{1+e^{-x}}$.\n",
    "\n",
    "So, we proceed by an iterative update:\n",
    "\n",
    "$$ \\alpha^{(n+1)} = \\alpha^{(n)} - \\frac{\\eta \\delta^{(n)}}{f'(\\alpha^{(n)}\\cdot x^{(n)}+\\beta^{(n)})} x^{(n)} $$\n",
    "\n",
    "where $\\delta^{(n)} = f(\\alpha^{(n)}\\cdot x^{(n)} + \\beta^{(n)}) - y^{(n)}$\n",
    "\n",
    "### An example\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "      <th>58</th>\n",
       "      <th>59</th>\n",
       "      <th>60</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0286</td>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>0.0174</td>\n",
       "      <td>0.0384</td>\n",
       "      <td>0.0990</td>\n",
       "      <td>0.1201</td>\n",
       "      <td>0.1833</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.3039</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0057</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0051</td>\n",
       "      <td>0.0062</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.0317</td>\n",
       "      <td>0.0956</td>\n",
       "      <td>0.1321</td>\n",
       "      <td>0.1408</td>\n",
       "      <td>0.1674</td>\n",
       "      <td>0.1710</td>\n",
       "      <td>0.0731</td>\n",
       "      <td>0.1401</td>\n",
       "      <td>0.2083</td>\n",
       "      <td>0.3513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0201</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0138</td>\n",
       "      <td>0.0092</td>\n",
       "      <td>0.0143</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0519</td>\n",
       "      <td>0.0548</td>\n",
       "      <td>0.0842</td>\n",
       "      <td>0.0319</td>\n",
       "      <td>0.1158</td>\n",
       "      <td>0.0922</td>\n",
       "      <td>0.1027</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>0.1465</td>\n",
       "      <td>0.2838</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0081</td>\n",
       "      <td>0.0120</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0097</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0375</td>\n",
       "      <td>0.0484</td>\n",
       "      <td>0.0475</td>\n",
       "      <td>0.0647</td>\n",
       "      <td>0.0591</td>\n",
       "      <td>0.0753</td>\n",
       "      <td>0.0098</td>\n",
       "      <td>0.0684</td>\n",
       "      <td>0.1487</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0128</td>\n",
       "      <td>0.0145</td>\n",
       "      <td>0.0058</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0059</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0173</td>\n",
       "      <td>0.0347</td>\n",
       "      <td>0.0070</td>\n",
       "      <td>0.0187</td>\n",
       "      <td>0.0671</td>\n",
       "      <td>0.1056</td>\n",
       "      <td>0.0697</td>\n",
       "      <td>0.0962</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>0.0179</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0068</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0       1       2       3       4       5       6       7       8   \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "5  0.0286  0.0453  0.0277  0.0174  0.0384  0.0990  0.1201  0.1833  0.2105   \n",
       "6  0.0317  0.0956  0.1321  0.1408  0.1674  0.1710  0.0731  0.1401  0.2083   \n",
       "7  0.0519  0.0548  0.0842  0.0319  0.1158  0.0922  0.1027  0.0613  0.1465   \n",
       "8  0.0223  0.0375  0.0484  0.0475  0.0647  0.0591  0.0753  0.0098  0.0684   \n",
       "9  0.0164  0.0173  0.0347  0.0070  0.0187  0.0671  0.1056  0.0697  0.0962   \n",
       "\n",
       "       9  ...      51      52      53      54      55      56      57      58  \\\n",
       "0  0.2111 ...  0.0027  0.0065  0.0159  0.0072  0.0167  0.0180  0.0084  0.0090   \n",
       "1  0.2872 ...  0.0084  0.0089  0.0048  0.0094  0.0191  0.0140  0.0049  0.0052   \n",
       "2  0.6194 ...  0.0232  0.0166  0.0095  0.0180  0.0244  0.0316  0.0164  0.0095   \n",
       "3  0.1264 ...  0.0121  0.0036  0.0150  0.0085  0.0073  0.0050  0.0044  0.0040   \n",
       "4  0.4459 ...  0.0031  0.0054  0.0105  0.0110  0.0015  0.0072  0.0048  0.0107   \n",
       "5  0.3039 ...  0.0045  0.0014  0.0038  0.0013  0.0089  0.0057  0.0027  0.0051   \n",
       "6  0.3513 ...  0.0201  0.0248  0.0131  0.0070  0.0138  0.0092  0.0143  0.0036   \n",
       "7  0.2838 ...  0.0081  0.0120  0.0045  0.0121  0.0097  0.0085  0.0047  0.0048   \n",
       "8  0.1487 ...  0.0145  0.0128  0.0145  0.0058  0.0049  0.0065  0.0093  0.0059   \n",
       "9  0.0251 ...  0.0090  0.0223  0.0179  0.0084  0.0068  0.0032  0.0035  0.0056   \n",
       "\n",
       "       59  60  \n",
       "0  0.0032   0  \n",
       "1  0.0044   0  \n",
       "2  0.0078   0  \n",
       "3  0.0117   0  \n",
       "4  0.0094   0  \n",
       "5  0.0062   0  \n",
       "6  0.0103   0  \n",
       "7  0.0053   0  \n",
       "8  0.0022   0  \n",
       "9  0.0040   0  \n",
       "\n",
       "[10 rows x 61 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonar = pd.read_csv(\"data/sonar.csv\",sep=\"\\t\",header=None)\n",
    "xs = sonar.iloc[:,0:60]\n",
    "ys = sonar.iloc[:,60]\n",
    "\n",
    "sonar.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff(f,x,eta):\n",
    "    return((f(x+eta/2)-f(x-eta/2))/eta)\n",
    "\n",
    "def sigmoid(x): return(1.0/(1.0+math.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(xs, ys, f, epochs, batches, eta, tol):\n",
    "    n = len(xs.iloc[0,:])\n",
    "    num = len(xs)\n",
    "    w = np.random.rand(n+1)\n",
    "    err = []\n",
    "    temp = 0.0\n",
    "    for i in range(epochs):\n",
    "        j = np.random.randint(num)\n",
    "        x = xs.iloc[j,:]\n",
    "        y = ys[j]\n",
    "        x0 = np.append([1],x)\n",
    "        x1 = np.dot(w,x0)\n",
    "        delta = f(x1) - y\n",
    "        if(i%batches == batches-1):\n",
    "            err.append(temp)\n",
    "            temp = 0.0\n",
    "        elif(abs(delta) > tol):\n",
    "            temp = temp + 1.0/batches\n",
    "        der = diff(f,x1,eta)+eta*np.random.rand()\n",
    "        w = w - (der*delta*eta)*x0\n",
    "    return(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1fbcb2b048>]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "errors = train(xs,ys,sigmoid,100000,1000,5e-1,0.45)\n",
    "plt.plot(errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks\n",
    "\n",
    "A [neural network](https://en.wikipedia.org/wiki/Artificial_neural_network) is a directed graph of perceptrons:\n",
    "\n",
    "![](images/neural-net.png)\n",
    "\n",
    "(Source: [Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network))\n",
    "\n",
    "In this case, the weights form a sequence of matrices (a tensor) $(w^i_{j,k})$.  In the feedforward phase of the network,  where the input $(x^i_j)$ at some layer $i$ is processed by the network as\n",
    "\n",
    "$$ x^{i+1}_k = \\sum_j f^i_k(x^i_j w^i_{j,k}) $$\n",
    "\n",
    "where $f^i_k$ is the activation function at the neuron $k$ at level $i$.\n",
    "\n",
    "There is a very large number of different types of neural networks.  You can find a good taxonomy of such networks [here](http://www.asimovinstitute.org/neural-network-zoo/):\n",
    "\n",
    "However, it is neither practical nor recommended that you implement neural networks by hand. \n",
    "\n",
    "![](images/meme.jpg)\n",
    "\n",
    "Use one of the following libraries or frameworks:\n",
    "\n",
    "1. [Theano](http://deeplearning.net/software/theano/)\n",
    "3. [TensorFlow](https://www.tensorflow.org/)\n",
    "4. [Caffe](http://caffe.berkeleyvision.org/) and [Caffe2](https://caffe2.ai/)\n",
    "2. [Keras](https://keras.io/)\n",
    "2. [MATLAB for Deep Learning](https://www.mathworks.com/campaigns/products/trials/targeted/dpl.html)\n",
    "5. [MXNet](https://mxnet.apache.org/)\n",
    "6. [The Microsoft Cognitive Toolkit](https://www.microsoft.com/en-us/cognitive-toolkit/)\n",
    "8. [Deep Learning for Java](https://deeplearning4j.org/)\n",
    "\n",
    "## Theano\n",
    "\n",
    "Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N = 400                                   \n",
    "feats = 784                               \n",
    "\n",
    "rng = numpy.random\n",
    "D = (rng.randn(N, feats), rng.randint(size=N, low=0, high=2))\n",
    "w = theano.shared(rng.randn(feats), name=\"w\")\n",
    "b = theano.shared(0., name=\"b\")\n",
    "\n",
    "x = T.dmatrix(\"x\")\n",
    "y = T.dvector(\"y\")\n",
    "\n",
    "p_1 = 1 / (1 + T.exp(-T.dot(x, w) - b))\n",
    "prediction = p_1 > 0.5      \n",
    "\n",
    "xent = -y * T.log(p_1) - (1-y) * T.log(1-p_1) \n",
    "cost = xent.mean() + 0.01 * (w ** 2).sum() \n",
    "\n",
    "gw, gb = T.grad(cost, [w, b])             \n",
    "                                          \n",
    "train = theano.function(\n",
    "          inputs=[x,y],\n",
    "          outputs=[prediction, xent],\n",
    "          updates=((w, w - 0.1 * gw), (b, b - 0.1 * gb)))\n",
    "\n",
    "training_steps = 10000\n",
    "for i in range(training_steps):\n",
    "    pred, err = train(D[0], D[1])\n",
    "\n",
    "predict = theano.function(inputs=[x], outputs=prediction)\n",
    "pred = predict(D[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       0\n",
      "True   1\n",
      "False  0\n",
      "True   1\n",
      "False  0\n",
      "True   1\n",
      "False  0\n",
      "False  0\n",
      "False  0\n",
      "False  0\n",
      "False  0\n",
      "False  0\n",
      "True   1\n",
      "False  0\n",
      "True   1\n",
      "True   1\n",
      "False  0\n",
      "False  0\n",
      "True   1\n",
      "False  0\n",
      "True   1\n",
      "The confusion matrix is:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[193,   0],\n",
       "       [  0, 207]])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame = pd.DataFrame(D[1],pred)\n",
    "print(frame.head(20))\n",
    "\n",
    "print(\"The confusion matrix is:\")\n",
    "confusion_matrix(D[1],pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras \n",
    "\n",
    "A simple neural-net for binary classification:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid', input_dim=60))\n",
    "#model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='RMSprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "208/208 [==============================] - 0s 27us/step - loss: 0.4035 - acc: 0.8125\n",
      "Epoch 2/100\n",
      "208/208 [==============================] - 0s 25us/step - loss: 0.4034 - acc: 0.8125\n",
      "Epoch 3/100\n",
      "208/208 [==============================] - 0s 25us/step - loss: 0.4033 - acc: 0.8125\n",
      "Epoch 4/100\n",
      "208/208 [==============================] - 0s 20us/step - loss: 0.4034 - acc: 0.8125\n",
      "Epoch 5/100\n",
      "208/208 [==============================] - 0s 37us/step - loss: 0.4033 - acc: 0.8125\n",
      "Epoch 6/100\n",
      "208/208 [==============================] - 0s 31us/step - loss: 0.4035 - acc: 0.8125\n",
      "Epoch 7/100\n",
      "208/208 [==============================] - 0s 16us/step - loss: 0.4033 - acc: 0.8125\n",
      "Epoch 8/100\n",
      "208/208 [==============================] - 0s 19us/step - loss: 0.4031 - acc: 0.8125\n",
      "Epoch 9/100\n",
      "208/208 [==============================] - 0s 59us/step - loss: 0.4035 - acc: 0.8221\n",
      "Epoch 10/100\n",
      "208/208 [==============================] - 0s 29us/step - loss: 0.4030 - acc: 0.8125\n",
      "Epoch 11/100\n",
      "208/208 [==============================] - 0s 21us/step - loss: 0.4038 - acc: 0.8077\n",
      "Epoch 12/100\n",
      "208/208 [==============================] - 0s 51us/step - loss: 0.4037 - acc: 0.8125\n",
      "Epoch 13/100\n",
      "208/208 [==============================] - 0s 27us/step - loss: 0.4029 - acc: 0.8125\n",
      "Epoch 14/100\n",
      "208/208 [==============================] - 0s 18us/step - loss: 0.4031 - acc: 0.8125\n",
      "Epoch 15/100\n",
      "208/208 [==============================] - 0s 16us/step - loss: 0.4028 - acc: 0.8125\n",
      "Epoch 16/100\n",
      "208/208 [==============================] - 0s 16us/step - loss: 0.4027 - acc: 0.8125\n",
      "Epoch 17/100\n",
      "208/208 [==============================] - 0s 16us/step - loss: 0.4027 - acc: 0.8125\n",
      "Epoch 18/100\n",
      "208/208 [==============================] - 0s 55us/step - loss: 0.4026 - acc: 0.8125\n",
      "Epoch 19/100\n",
      "208/208 [==============================] - 0s 36us/step - loss: 0.4027 - acc: 0.8125\n",
      "Epoch 20/100\n",
      "208/208 [==============================] - 0s 15us/step - loss: 0.4030 - acc: 0.8125\n",
      "Epoch 21/100\n",
      "208/208 [==============================] - 0s 16us/step - loss: 0.4028 - acc: 0.8221\n",
      "Epoch 22/100\n",
      "208/208 [==============================] - 0s 15us/step - loss: 0.4025 - acc: 0.8125\n",
      "Epoch 23/100\n",
      "208/208 [==============================] - 0s 13us/step - loss: 0.4025 - acc: 0.8125\n",
      "Epoch 24/100\n",
      "208/208 [==============================] - 0s 14us/step - loss: 0.4030 - acc: 0.8125\n",
      "Epoch 25/100\n",
      "208/208 [==============================] - 0s 26us/step - loss: 0.4025 - acc: 0.8125\n",
      "Epoch 26/100\n",
      "208/208 [==============================] - 0s 83us/step - loss: 0.4024 - acc: 0.8173\n",
      "Epoch 27/100\n",
      "208/208 [==============================] - 0s 34us/step - loss: 0.4030 - acc: 0.8125\n",
      "Epoch 28/100\n",
      "208/208 [==============================] - 0s 24us/step - loss: 0.4023 - acc: 0.8125\n",
      "Epoch 29/100\n",
      "208/208 [==============================] - 0s 14us/step - loss: 0.4023 - acc: 0.8125\n",
      "Epoch 30/100\n",
      "208/208 [==============================] - 0s 63us/step - loss: 0.4029 - acc: 0.8125\n",
      "Epoch 31/100\n",
      "208/208 [==============================] - 0s 19us/step - loss: 0.4021 - acc: 0.8125\n",
      "Epoch 32/100\n",
      "208/208 [==============================] - 0s 22us/step - loss: 0.4021 - acc: 0.8125\n",
      "Epoch 33/100\n",
      "208/208 [==============================] - 0s 17us/step - loss: 0.4021 - acc: 0.8125\n",
      "Epoch 34/100\n",
      "208/208 [==============================] - 0s 17us/step - loss: 0.4021 - acc: 0.8221\n",
      "Epoch 35/100\n",
      "208/208 [==============================] - 0s 15us/step - loss: 0.4022 - acc: 0.8173\n",
      "Epoch 36/100\n",
      "208/208 [==============================] - 0s 16us/step - loss: 0.4019 - acc: 0.8125\n",
      "Epoch 37/100\n",
      "208/208 [==============================] - 0s 41us/step - loss: 0.4019 - acc: 0.8125\n",
      "Epoch 38/100\n",
      "208/208 [==============================] - 0s 33us/step - loss: 0.4019 - acc: 0.8173\n",
      "Epoch 39/100\n",
      "208/208 [==============================] - 0s 18us/step - loss: 0.4022 - acc: 0.8125\n",
      "Epoch 40/100\n",
      "208/208 [==============================] - 0s 37us/step - loss: 0.4018 - acc: 0.8173\n",
      "Epoch 41/100\n",
      "208/208 [==============================] - 0s 89us/step - loss: 0.4017 - acc: 0.8173\n",
      "Epoch 42/100\n",
      "208/208 [==============================] - 0s 21us/step - loss: 0.4019 - acc: 0.8173\n",
      "Epoch 43/100\n",
      "208/208 [==============================] - 0s 82us/step - loss: 0.4017 - acc: 0.8125\n",
      "Epoch 44/100\n",
      "208/208 [==============================] - 0s 20us/step - loss: 0.4015 - acc: 0.8125\n",
      "Epoch 45/100\n",
      "208/208 [==============================] - 0s 21us/step - loss: 0.4020 - acc: 0.8077\n",
      "Epoch 46/100\n",
      "208/208 [==============================] - 0s 46us/step - loss: 0.4014 - acc: 0.8125\n",
      "Epoch 47/100\n",
      "208/208 [==============================] - 0s 47us/step - loss: 0.4016 - acc: 0.8173\n",
      "Epoch 48/100\n",
      "208/208 [==============================] - 0s 17us/step - loss: 0.4014 - acc: 0.8125\n",
      "Epoch 49/100\n",
      "208/208 [==============================] - 0s 19us/step - loss: 0.4019 - acc: 0.8125\n",
      "Epoch 50/100\n",
      "208/208 [==============================] - 0s 52us/step - loss: 0.4018 - acc: 0.8125\n",
      "Epoch 51/100\n",
      "208/208 [==============================] - 0s 31us/step - loss: 0.4013 - acc: 0.8125\n",
      "Epoch 52/100\n",
      "208/208 [==============================] - 0s 16us/step - loss: 0.4012 - acc: 0.8125\n",
      "Epoch 53/100\n",
      "208/208 [==============================] - 0s 16us/step - loss: 0.4015 - acc: 0.8125\n",
      "Epoch 54/100\n",
      "208/208 [==============================] - 0s 16us/step - loss: 0.4011 - acc: 0.8125\n",
      "Epoch 55/100\n",
      "208/208 [==============================] - 0s 18us/step - loss: 0.4016 - acc: 0.8125\n",
      "Epoch 56/100\n",
      "208/208 [==============================] - 0s 69us/step - loss: 0.4010 - acc: 0.8173\n",
      "Epoch 57/100\n",
      "208/208 [==============================] - 0s 19us/step - loss: 0.4010 - acc: 0.8125\n",
      "Epoch 58/100\n",
      "208/208 [==============================] - 0s 17us/step - loss: 0.4014 - acc: 0.8125\n",
      "Epoch 59/100\n",
      "208/208 [==============================] - 0s 27us/step - loss: 0.4012 - acc: 0.8173\n",
      "Epoch 60/100\n",
      "208/208 [==============================] - 0s 16us/step - loss: 0.4012 - acc: 0.8173\n",
      "Epoch 61/100\n",
      "208/208 [==============================] - 0s 16us/step - loss: 0.4011 - acc: 0.8269\n",
      "Epoch 62/100\n",
      "208/208 [==============================] - 0s 22us/step - loss: 0.4009 - acc: 0.8221\n",
      "Epoch 63/100\n",
      "208/208 [==============================] - 0s 18us/step - loss: 0.4009 - acc: 0.8221\n",
      "Epoch 64/100\n",
      "208/208 [==============================] - 0s 20us/step - loss: 0.4007 - acc: 0.8125\n",
      "Epoch 65/100\n",
      "208/208 [==============================] - 0s 17us/step - loss: 0.4009 - acc: 0.8125\n",
      "Epoch 66/100\n",
      "208/208 [==============================] - 0s 15us/step - loss: 0.4008 - acc: 0.8173\n",
      "Epoch 67/100\n",
      "208/208 [==============================] - 0s 15us/step - loss: 0.4012 - acc: 0.8173\n",
      "Epoch 68/100\n",
      "208/208 [==============================] - 0s 14us/step - loss: 0.4007 - acc: 0.8125\n",
      "Epoch 69/100\n",
      "208/208 [==============================] - 0s 14us/step - loss: 0.4005 - acc: 0.8125\n",
      "Epoch 70/100\n",
      "208/208 [==============================] - 0s 45us/step - loss: 0.4009 - acc: 0.8125\n",
      "Epoch 71/100\n",
      "208/208 [==============================] - 0s 32us/step - loss: 0.4009 - acc: 0.8125\n",
      "Epoch 72/100\n",
      "208/208 [==============================] - 0s 40us/step - loss: 0.4005 - acc: 0.8269\n",
      "Epoch 73/100\n",
      "208/208 [==============================] - 0s 20us/step - loss: 0.4009 - acc: 0.8125\n",
      "Epoch 74/100\n",
      "208/208 [==============================] - 0s 22us/step - loss: 0.4003 - acc: 0.8125\n",
      "Epoch 75/100\n",
      "208/208 [==============================] - 0s 18us/step - loss: 0.4004 - acc: 0.8125\n",
      "Epoch 76/100\n",
      "208/208 [==============================] - 0s 19us/step - loss: 0.4006 - acc: 0.8173\n",
      "Epoch 77/100\n",
      "208/208 [==============================] - 0s 30us/step - loss: 0.4002 - acc: 0.8125\n",
      "Epoch 78/100\n",
      "208/208 [==============================] - 0s 19us/step - loss: 0.4005 - acc: 0.8125\n",
      "Epoch 79/100\n",
      "208/208 [==============================] - 0s 21us/step - loss: 0.4004 - acc: 0.8125\n",
      "Epoch 80/100\n",
      "208/208 [==============================] - 0s 18us/step - loss: 0.4003 - acc: 0.8221\n",
      "Epoch 81/100\n",
      "208/208 [==============================] - 0s 16us/step - loss: 0.4002 - acc: 0.8173\n",
      "Epoch 82/100\n",
      "208/208 [==============================] - 0s 73us/step - loss: 0.4005 - acc: 0.8221\n",
      "Epoch 83/100\n",
      "208/208 [==============================] - 0s 21us/step - loss: 0.3999 - acc: 0.8269\n",
      "Epoch 84/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208/208 [==============================] - 0s 17us/step - loss: 0.4007 - acc: 0.8221\n",
      "Epoch 85/100\n",
      "208/208 [==============================] - 0s 18us/step - loss: 0.3999 - acc: 0.8269\n",
      "Epoch 86/100\n",
      "208/208 [==============================] - 0s 19us/step - loss: 0.4000 - acc: 0.8221\n",
      "Epoch 87/100\n",
      "208/208 [==============================] - 0s 83us/step - loss: 0.4000 - acc: 0.8173\n",
      "Epoch 88/100\n",
      "208/208 [==============================] - 0s 20us/step - loss: 0.4005 - acc: 0.8173\n",
      "Epoch 89/100\n",
      "208/208 [==============================] - 0s 43us/step - loss: 0.3999 - acc: 0.8125\n",
      "Epoch 90/100\n",
      "208/208 [==============================] - 0s 25us/step - loss: 0.3999 - acc: 0.8125\n",
      "Epoch 91/100\n",
      "208/208 [==============================] - 0s 18us/step - loss: 0.3997 - acc: 0.8125\n",
      "Epoch 92/100\n",
      "208/208 [==============================] - 0s 26us/step - loss: 0.3997 - acc: 0.8125\n",
      "Epoch 93/100\n",
      "208/208 [==============================] - 0s 22us/step - loss: 0.4006 - acc: 0.8077\n",
      "Epoch 94/100\n",
      "208/208 [==============================] - 0s 57us/step - loss: 0.3997 - acc: 0.8125\n",
      "Epoch 95/100\n",
      "208/208 [==============================] - 0s 27us/step - loss: 0.3995 - acc: 0.8125\n",
      "Epoch 96/100\n",
      "208/208 [==============================] - 0s 46us/step - loss: 0.3996 - acc: 0.8125\n",
      "Epoch 97/100\n",
      "208/208 [==============================] - 0s 30us/step - loss: 0.3995 - acc: 0.8125\n",
      "Epoch 98/100\n",
      "208/208 [==============================] - 0s 21us/step - loss: 0.3997 - acc: 0.8125\n",
      "Epoch 99/100\n",
      "208/208 [==============================] - 0s 107us/step - loss: 0.3997 - acc: 0.8173\n",
      "Epoch 100/100\n",
      "208/208 [==============================] - 0s 19us/step - loss: 0.3995 - acc: 0.8173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1fba164c50>"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(xs, ys, epochs=100, batch_size=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
